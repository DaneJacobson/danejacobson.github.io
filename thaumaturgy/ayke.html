<!doctype html>
<html lang="en-us">
<head>
    <!-- Cloudflare Web Analytics -->
    <script defer 
        src='https://static.cloudflareinsights.com/beacon.min.js'
        data-cf-beacon='{"token": "95b41080581648879f87b1dc54c0c376"}'>
    </script>
    <!-- End Cloudflare Web Analytics -->

    <meta name="generator" content="Hugo 0.114.0">
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="IE=edge">
    <title>Dane Jacobson | Ayke Logbook</title>
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="robots" content="all,follow">
    <meta name="googlebot" content="index,follow,snippet,archive">
    <link rel="stylesheet" href="../css/terminal-0.7.2.min.css">
    <link rel="stylesheet" href="../css/animate-4.1.1.min.css">
    <link rel="stylesheet" href="../css/console.css">
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
        <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <link rel="apple-touch-con" sizes="180x180" href="../img/favicon/profile.jpg">
    <link rel="icon" type="image/jpg" sizes="32x32" href="../img/favicon/profile.jpg">
    <link rel="icon" type="image/jpg" sizes="16x16" href="../img/favicon/profile.jpg">
    <link rel="manifest" href="/img/favicon/site.webmanifest">
    <link rel="shortcut icon" href="/img/favicon/favicon.ico">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-config" content="/img/favicon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    <link href="https://danej.co/index.xml" rel="alternate" type="application/rss+xml" title="Ayke Logbook">
    <meta property="og:title" content="Ayke Logbook">
    <meta property="og:description" content>
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://danej.co/">
    <meta name="twitter:title" content="Ayke Logbook">
    <meta name="twitter:description" content>
</head>
<body class="terminal">
    <div class="container">
        <div class="terminal-nav">
            <header class="terminal-logo">
                <div class="logo terminal-prompt">
                    <a href="../">danejacobson</a>
                    :~#
                    <a href="/thaumaturgy/">thaumaturgy</a>
                    /
                    <a href="/thaumaturgy/ayke">ayke</a>
                    /
                </div>
            </header>
            <nav class="terminal-menu">
                <ul vocab="https://schema.org/" typeof="BreadcrumbList">
                    <li><a href="/notes/" typeof="ListItem">notes/</a></li>
                    <li><a href="/vitaegraph/" typeof="ListItem">vitaegraph/</a></li>
                    <li><a href="/thaumaturgy/" typeof="ListItem">thaumaturgy/</a></li>
                </ul>
            </nav>
        </div>
    </div>
    <div class="container animated zoomIn fast">
        <h1>Ayke Logbook</h1>
        <br>

        <p>
            <a href="https://en.wikipedia.org/wiki/Ayke_Agus">Ayke Agus</a> 
            was one of 
            <a href="https://www.youtube.com/watch?v=kFaq9kTlcaY">Jascha Heifetz</a>'s 
            friends, accompanists, and biographer.
            She was not only a virtuoso, but a deeply thoughtful pianist, with an
            ability to read musician's minds.
            Also she Indonesian, just like my mother! The North Star
            of Ayke is no less than an AI pianist that can:
        </p>
        <ol>
            <li>Play any piece</li>
            <li>With any instrument</li>
            <li>Follow the soloist</li>
            <li>Live and in real time</li>
        </ol>
        <p>
            Imagine a pianist at a hundredth of the cost, that can 
            play forever, at any time of day, at any place. This is the promise of
            Ayke, and as God is my witness I shall deliver for every child who
            can't afford or doesn't have access to a pianist.
        </p>
        <p>
            This is my logbook (which is in reverse chronological order because 
            I choose the rules). I'm learning everything on-demand, so if you're 
            inspired know that I, too, started from nothing. You can just do things!
        </p>

        <h2>25-07-2024</h2>
        <p>
            I ran into <a href="https://www.linkedin.com/in/anirudh-mani-1796934b/">Anirudh Mani</a> 
            again at the Back Bay WeWork and he was fantastically helpful chatting
            about Ayke. By the way, he's building <a href="https://www.lemonaide.ai/">lemonaide.ai</a> which
            is an AI copilot for musicians that lives in your studio! Ani is also passionate about  
            ethically sourcing musical training data, and has made me rethink my prior assumptions about
            musicians'/companies' rights regarding training data use. They have a big product release 
            dropping on August 8th (yes, 808, get it), so tune in!
        </p>
        <p>
            Anyway, I have combined violin+piano data, and he suggested that the most important component
            was stem splitting and getting a data extraction pipeline running, so instead of thinking 
            too much about architectures, I'm going to start with preprocessing the data. He also said 
            that all the components I'm thinking of using are in the literature so I really just need
            to roll up my sleeves and dive in. Thanks Ani!
        </p>

        <h2>24-07-2024</h2>
        <p>
            Using SunoAI's Bark as a reference, which has 3 transformers, text->semantic, semantic->coarse-grained,
            and coarse->fine. Ok, rethinking this, there are no rules, I don't need to listen to anybody.
            I'm going to follow my gut and if it's wrong, then it's wrong. I'll read the more foundational
            papers on the train to work, Encodec, Mujoco, MusicLLM, etc., and start super small. Ran 
            out of time today, but I will train a simple piano audio transformer on a Chopin piece and 
            observe what the result is.
        </p>

        <h2>23-07-2024</h2>
        <p>
            Conceived of the project and set up the Ayke Logbook. Pretty obvious 
            that machine learning is required here, and since big autoregressive 
            models are killing it I'm just going to go for that approach. Did some brainstorming
            on what a foundational model might look like. Since I am a violinist
            we might as well 
            <a href="https://www.uservoice.com/blog/drive-internal-alignment">dogfood</a> 
            and start with just a violin accompaniment.
        </p>
        <p>
            At a first pass, the final model takes violin audio, piano audio, and 
            sheet music, just like how humans do it. Now for v0, I suspect that sheet music is irrelevant
            and that we only need the violin and piano audio. I'm thinking we pick one short song (TBD, 
            but don't worry the hidden ace up my sleeve with Ayke is I have lots of data) and train 
            some autoregressive model to predict what the next sampling step is from the current audio.
            Then at inference time we have no violin output, and the piano output is both played out 
            loud and passed back in as input with a context window (along with the live violin audio). 
            This sounds stupid but I do not care. 
            At the very least I'll get the audio processing basics down.
            Fail fast, let's go!
        </p>
        <p>
            Steps: split violin and piano audio and make a big dataset. Define architecture and train.
            Attempt inference. Profit.
        </p>

        

        <div class="footer">
            Modified from 
            <a href="https://github.com/mrmierzejewski/hugo-theme-console/" target="_blank">
                Hugo Console Theme
            </a>
        </div>
    </div>
</body>
</html>
